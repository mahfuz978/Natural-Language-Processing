{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR6t1EvNcV+IyMGl621V+L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahfuz978/TECH-I.S.-----NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWpQHTaOm8xD",
        "outputId": "6a3a5692-a3de-44c8-fc3f-454f75bc3edd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9eyzPOC1tcT",
        "outputId": "19ab2200-9547-4e65-df1f-a13cdd1d695c"
      },
      "source": [
        "paragraph = \"\"\"Hello everyone. Welcome to Tech IS. Tech IS is a global prograpping school. We are from Silicon Valley, USA.\n",
        "           Our Tutors are Professional engineers with extensive work experience in information Technology Industry hailing from around the world.\n",
        "           You are studying NLP article.\"\"\"\n",
        "\n",
        "# Tokenizing sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello everyone.', 'Welcome to Tech IS.', 'Tech IS is a global prograpping school.', 'We are from Silicon Valley, USA.', 'Our Tutors are Professional engineers with extensive work experience in information Technology Industry hailing from around the world.', 'You are studying NLP article.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-O52oc33Oek",
        "outputId": "a918b77f-68e6-41d4-8ca2-3c49770c3236"
      },
      "source": [
        "# Tokenizing words\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "print(words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'everyone', '.', 'Welcome', 'to', 'Tech', 'IS', '.', 'Tech', 'IS', 'is', 'a', 'global', 'prograpping', 'school', '.', 'We', 'are', 'from', 'Silicon', 'Valley', ',', 'USA', '.', 'Our', 'Tutors', 'are', 'Professional', 'engineers', 'with', 'extensive', 'work', 'experience', 'in', 'information', 'Technology', 'Industry', 'hailing', 'from', 'around', 'the', 'world', '.', 'You', 'are', 'studying', 'NLP', 'article', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPuSKzBL3jX9",
        "outputId": "43af05fc-0e48-41fe-f906-c366ff757cfd"
      },
      "source": [
        "# Identifying Stop Words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1Tt0jZO39Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446c2682-7a91-46d5-a505-292203858fe3"
      },
      "source": [
        "dir(nltk.stem)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ISRIStemmer',\n",
              " 'LancasterStemmer',\n",
              " 'PorterStemmer',\n",
              " 'RSLPStemmer',\n",
              " 'RegexpStemmer',\n",
              " 'SnowballStemmer',\n",
              " 'StemmerI',\n",
              " 'WordNetLemmatizer',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'api',\n",
              " 'isri',\n",
              " 'lancaster',\n",
              " 'porter',\n",
              " 'regexp',\n",
              " 'rslp',\n",
              " 'snowball',\n",
              " 'util',\n",
              " 'wordnet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRlc079a9nWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b3e94a-f10f-4d75-9b62-c3a94ef84928"
      },
      "source": [
        "# stemming is to convert words into its base root form.\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "sentences =nltk.sent_tokenize(paragraph)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words \n",
        "           if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "print(sentences)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello everyon .', 'welcom tech IS .', 'tech IS global prograp school .', 'We silicon valley , usa .', 'our tutor profession engin extens work experi inform technolog industri hail around world .', 'you studi nlp articl .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRTJsXiqhcyz",
        "outputId": "40df48f9-6b2b-48f3-ea45-75407a853666"
      },
      "source": [
        "len(set(stopwords.words('english')))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXZioxuOxHPg",
        "outputId": "8f166d45-c749-4b32-a178-f682f2eb0700"
      },
      "source": [
        "# lemmatization is to group iflected forms of the word\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word) for word in words \n",
        "         if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "print(sentences)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['Hello everyone .', 'Welcome Tech IS .', 'Tech IS global prograpping school .', 'We Silicon Valley , USA .', 'Our Tutors Professional engineer extensive work experience information Technology Industry hailing around world .', 'You studying NLP article .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bacf02eR7wTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2539945f-0de8-409e-bfae-b16612e91ab8"
      },
      "source": [
        "import re\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n",
        "\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            " [1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BXHm5uwkAA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5287d19-29d1-4102-f3e9-cf7d440b462e"
      },
      "source": [
        "# cleaning the text and create corpus\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.70710678 0.         0.\n",
            "  0.         0.         0.70710678 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.6340862\n",
            "  0.         0.         0.         0.         0.77326237 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.52182349 0.         0.         0.         0.         0.\n",
            "  0.         0.52182349 0.52182349 0.         0.         0.42790272\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.57735027 0.         0.\n",
            "  0.         0.         0.57735027 0.57735027 0.         0.\n",
            "  0.        ]\n",
            " [0.28867513 0.         0.28867513 0.         0.28867513 0.28867513\n",
            "  0.         0.28867513 0.         0.28867513 0.28867513 0.\n",
            "  0.28867513 0.         0.         0.         0.         0.\n",
            "  0.28867513 0.28867513 0.         0.         0.         0.28867513\n",
            "  0.28867513]\n",
            " [0.         0.57735027 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.57735027\n",
            "  0.         0.         0.         0.         0.57735027 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIXCn-GvqVpk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}